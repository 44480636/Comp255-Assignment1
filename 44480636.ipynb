{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal with dataset 1\n",
      "deal with dataset 2\n",
      "deal with dataset 3\n",
      "deal with dataset 4\n",
      "deal with dataset 5\n",
      "deal with dataset 6\n",
      "deal with dataset 7\n",
      "deal with dataset 8\n",
      "deal with dataset 9\n",
      "deal with dataset 10\n",
      "deal with dataset 11\n",
      "deal with dataset 12\n",
      "deal with dataset 13\n",
      "deal with dataset 14\n",
      "deal with dataset 15\n",
      "deal with dataset 16\n",
      "deal with dataset 17\n",
      "deal with dataset 18\n",
      "deal with dataset 19\n",
      "Accuracy:  0.9241842610364683\n",
      "[[ 57   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0  57   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  56   1   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   2  97   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   1  47   5   1   2   0   0   0   0   0]\n",
      " [  0   0   0   2  17  65   0   1   0   0   0   0   0]\n",
      " [  0   0   2   0   2   1 212   1   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   5  33   0   0   1   0   0]\n",
      " [  0   0   0   0   0   1   1   0  36   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0  95   0   0   0]\n",
      " [  0   0   0   0   0   1   1   0   0   0  83  15   0]\n",
      " [  0   0   0   0   0   0   1   0   1   0  11  87   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  38]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = None\n",
    "#read file from dataset\n",
    "def load_dataset(i):\n",
    "    df = pd.read_csv('dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "\n",
    "def data_visulization(i,c,sensor): \n",
    "    df = pd.read_csv('dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "    activity_data = df[df[24] == c].values\n",
    "    if sensor == 'wrist-a':\n",
    "        t = 0\n",
    "    elif sensor == 'wrist-g':\n",
    "        t = 1\n",
    "    elif sensor == 'chest-a':\n",
    "        t = 2\n",
    "    elif sensor == 'chest-g':\n",
    "        t = 3\n",
    "    elif sensor == 'hip-a':\n",
    "        t = 4\n",
    "    elif sensor == 'hip-g':\n",
    "        t = 5\n",
    "    elif sensor == 'ankle-a':\n",
    "        t = 6\n",
    "    elif sensor == 'ankle-g':\n",
    "        t = 7\n",
    "    plt.plot(activity_data[:, 0+3*t : 3+3*t])\n",
    "    plt.show()\n",
    "        \n",
    "def noise_removing(i,c):\n",
    "    df = pd.read_csv('dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "    b, a = signal.butter(4, 0.04, 'lowpass', analog=False)\n",
    "    activity_data = df[df[24] == c].values\n",
    "    for n in range(24):\n",
    "        activity_data[:,n] = signal.lfilter(b, a, activity_data[:, n])\n",
    "\n",
    "#the function append feature from data to feature array\n",
    "def sample(arr,sample_data):\n",
    "    for i in range(24):\n",
    "        arr.append(np.min(sample_data[:, i]))\n",
    "        arr.append(np.max(sample_data[:, i]))\n",
    "        arr.append(np.mean(sample_data[:, i]))\n",
    "        arr.append(np.average(sample_data[:, i]))\n",
    "        arr.append(np.median(sample_data[:, i]))\n",
    "    arr.append(sample_data[0, -1])\n",
    "    arr = np.array([arr])\n",
    "    return arr\n",
    "\n",
    "def feature_engineering():\n",
    "    training = np.empty(shape=(0, 121))\n",
    "    testing = np.empty(shape=(0, 121))\n",
    "    for i in range(19):\n",
    "        df = pd.read_csv('dataset_' + str(i + 1) + '.txt', sep=',', header=None)\n",
    "        print('deal with dataset ' + str(i + 1))\n",
    "        for c in range(1, 14):\n",
    "            activity_data = df[df[24] == c].values\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "                \n",
    "            datat_len = len(activity_data)\n",
    "            training_len = math.floor(datat_len * 0.8)\n",
    "            training_data = activity_data[:training_len, :]\n",
    "            testing_data = activity_data[training_len:, :]\n",
    "            \n",
    "            training_sample_number = training_len // 1000 + 1\n",
    "            testing_sample_number = (datat_len - training_len) // 1000 + 1\n",
    "            \n",
    "            for s in range(training_sample_number):\n",
    "                if s < training_sample_number - 1:\n",
    "                    sample_data = training_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_data = training_data[1000*s:, :]\n",
    "            \n",
    "                feature_sample = []\n",
    "                feature_sample = sample(feature_sample,sample_data)\n",
    "                training = np.concatenate((training, feature_sample), axis=0)\n",
    "                \n",
    "            for s in range(testing_sample_number):\n",
    "                if s < testing_sample_number - 1:\n",
    "                    sample_data = testing_data[1000*s:1000*(s + 1), :]\n",
    "                else:\n",
    "                    sample_data = testing_data[1000*s:, :]\n",
    "\n",
    "                feature_sample = []\n",
    "                feature_sample = sample(feature_sample,sample_data)\n",
    "                testing = np.concatenate((testing, feature_sample), axis=0)\n",
    "                \n",
    "    df_training = pd.DataFrame(training)\n",
    "    df_testing = pd.DataFrame(testing)\n",
    "    df_training.to_csv('training_data.csv', index=None, header=None)\n",
    "    df_testing.to_csv('testing_data.csv', index=None, header=None)\n",
    "    \n",
    "def model_training_and_evaluation():\n",
    "    df_training = pd.read_csv('training_data.csv', header=None)\n",
    "    df_testing = pd.read_csv('testing_data.csv', header=None)\n",
    "    \n",
    "    #training input\n",
    "    y_train = df_training[df_training.shape[1] - 1].values - 1\n",
    "    y_train = y_train.astype(int)\n",
    "    df_training = df_training.drop([df_training.shape[1] - 1], axis=1)\n",
    "    X_train = df_training.values\n",
    "    \n",
    "    #testing input\n",
    "    y_test = df_testing[df_testing.shape[1] - 1].values - 1\n",
    "    y_test = y_test.astype(int)\n",
    "    df_testing = df_testing.drop([df_testing.shape[1] - 1], axis=1)\n",
    "    X_test = df_testing.values\n",
    "    \n",
    "    #normalization method\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #Using KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #data_visulization(i,c,'sensor')\n",
    "    #sensor: wrist-a, wrist-g, chest-a, chest-g, hip-a, hip-g, ankle-a, ankle-g\n",
    "    feature_engineering()\n",
    "    model_training_and_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
